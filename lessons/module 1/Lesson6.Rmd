---
title: "Урок шестой: Статистика: корреляция, регрессия и линейные модели"
author: "Вячеслав Арбузов"
date: "`r Sys.Date()`"
output: html_document
---

## Корреляционый анализ

Корреляционный анализ - статистический метод позволяющий определить существует ли зависимость между переменными и на сколько она сильна. 
### Парная корреляция 
Коэффициент корреляции -- это мера линейной связи между двумя переменными Параметрические (Пирсон) и непараметрические коэффициенты (Спирмена, Кендала) корреляции. Непараметрические устойчивы к выбросам.

```{r}
# создадим линейную зависимость с шумом
x = rnorm(70)
e = rnorm(70)
y = 3*x + 2*e
# визуализируем
plot(x,y)

# корреляция Пирсона
cor(x,y) #pearson
# корреляция Спирмена
cor(x,y,method = 'spearman')
# проверяем значимость коэффициента корреляции
cor.test(x, y)

# добавим выбросов в данные
y = c(y,16)
x = c(x,-12)
y = c(y,-16)
x = c(x,12)
plot(x,y)

# корреляция Пирсона не устойчива к выборосам, изменился даже знак зависимости
cor(x,y) #pearson
# корреляция Спирмена - непараметрический метод устойчивый к выбросам
cor(x,y,method = 'spearman')
```

### Матрица корреляций и ее визуализация

Корреляционная матрица (матрица корреляций) -- это квадратная таблица, заголовками строк и столбцов которой являются обрабатываемые переменные, а на пересечении строк и столбцов выводятся коэффициенты корреляции для соответствующей пары признаков. Рассмотрим разные методы анализа и визуализации корреляционной матрицы.

```{r}
#загрузим данные mtcars
data(mtcars)
str(mtcars)
# построим корреляционную матрицу и округлим до 2 знаков
M_COR = round(cor(mtcars),2)

# метод 1
# View(M_COR)

# метод 2 - с использованием библиотеки corrplot
library(corrplot)
corrplot(M_COR,method = 'circle')
corrplot(M_COR,method = 'square')
corrplot(M_COR,method = 'ellipse')
corrplot(M_COR,method = 'number')
# можно упорядочить с помощью order и убрать часть матрицы
corrplot(M_COR,method = 'square',order = 'hclust',type = 'lower')
corrplot(M_COR,method = 'color')
corrplot(M_COR,method = 'pie')

# метод 3 - с использованием библиотеки PerformanceAnalytics
library(PerformanceAnalytics)
chart.Correlation(mtcars[,1:4])
```

## Регрессионый анализ

Зависимая переменная - то, что мы измеряем в эксперименте: цена акции, количество пользователей, правильность ответа и т.д. Предиктор (predictor) - переменная в фокусе нашего интереса. Наша главная задача - узнать, имеет ли она какое-то отношение к зависимой переменной. Может ли изменение предиктора привести к изменению в зависимой переменной? Intercept - свободный коэффициент; то, чему равна зависимая переменная, если предиктор равен нулю Slope - угол наклона прямой; показывает, насколько изменяется зависимая переменная при изменении предиктора

### Линейная регрессия

Линейная регрессия предоставляет собой лучшую линию (поверхность в многомерном случае ), которую можно провести через данные. У лучшей модели минимальные остатки Требования к остаткам (распределению ошибок):

-   линейная зависимость зависимой переменной от предиктора

-   нормальное распределение остатков

-   равномерная дисперсия остатков

-   независимость остатков

```{r}
# построим парную регрессию
# для построенния линейной модели используем функцию lm
# указываем формулу линейной модели в качестве одного из аргумента
model1 = lm(mpg~hp,data = mtcars)
# для более подробной информации используем функцию summary
model1_sum = summary(model1)
model1_sum

# проверим на нормальность остатки
library(nortest)
nortest::lillie.test(model1$residuals)

# строим многофакторную регрессию
model1 = lm(mpg~hp + drat,data = mtcars)
model1_sum = summary(model1)
# R^2 модели
model1_sum$adj.r.squared

# построим модель от всех переменных
# такой подход обычно не дает сразу хороших результатов
model_full = lm(mpg ~ .,data = mtcars)
model_full_sum = summary(model_full)
model_full_sum

# можно удалить не нужные переменные
model_minus = lm(mpg ~ . -cyl -gear - carb,data = mtcars)
summary(model_minus)

# можно выбрать оптимальную модель с помощью функции step
best_model = step(model_full,direction = 'backward',trace = 0)
summary(best_model)

# можно рассматривать переменные как числовые или факторные
# результат будет разный
model3 = lm(mpg ~ cyl,data = mtcars)
summary(model3)
# количество цилиндров - это факторная переменная
mtcars$cyl = factor(mtcars$cyl)
model4 = lm(mpg ~ cyl,data = mtcars)
summary(model4)

# можно убрать свободное слагаемое при оценке модели
model5 = lm(mpg ~ 0 + wt+qsec,data = mtcars)
summary(model5)

# нелинейные части в регрессии
model6 = lm(mpg ~ I(hp^0.1) + wt,data = mtcars)
summary(model6)

# модель с взаимодействием переменных
model7 = lm(mpg ~ hp+ wt:cyl ,data = mtcars)
summary(model7)
# сами факторы с взаимодействием
model8 = lm(mpg ~ wt*cyl ,data = mtcars)
summary(model8)

```

### Прогнозирование

Для прогнозирования в R используют функцию 'predict'

```{r}
# указываем модель и новые данные на основе которых хотим спрогнозировать результат
predict(model4,newdata = mtcars[3,])
#несколько наблюдений
predict(model4,newdata = mtcars[3:8,])
```

### Обобщеные модели

Более широкий класс линейных моделей Распределение ошибок в них отличается от нормального распределения В зависимости от результирующей переменной выделают следующие типы моделей

-   Регрессия Пуассона

-   Мультиномиальная регрессия

-   Биномиальная (бинарного выбора) регрессия

    -   Логит модель

    -   Пробит модель

```{r}

# для построения большинства обобщенных моделей используется функция glm
# построим логит модель
logit_model <- glm(vs ~ qsec, data=mtcars,family = binomial(link = logit))
summary(logit_model)
predict(logit_model,mtcars[1:3,])

# построим пробит модель
probit_model <- glm(vs ~ qsec, data=mtcars,family = binomial(link = probit))
summary(probit_model)
# в случае моделей бинарного выбора можно указать тип response для прогнозирования вероятности
predict(probit_model,mtcars[1:3,],type='response')

# исключение составляет мультиномиальная регрессия
library(nnet)
mult_model <- multinom(as.factor(cyl) ~ qsec, data= mtcars)
summary(mult_model)
predict(mult_model,mtcars[1:3,])

```
